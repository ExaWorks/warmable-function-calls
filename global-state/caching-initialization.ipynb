{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d750f36-1d9b-40de-babb-0bc3544e9b52",
   "metadata": {},
   "source": [
    "# Initialize Once\n",
    "It is possible to cache initialization in a way that will save it across multiple runs of a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b086c718-ea41-4999-b149-d728d4522800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep  # We use this to fake larger functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e0d539-81fd-4081-8a40-0c31b13cfddd",
   "metadata": {},
   "source": [
    "## Part One: Make the function\n",
    "A good pattern for the \"warmable\" function is to describe the initialization part and the computation part into separate functions. That will make it easy to mark which function to cache. \n",
    "\n",
    "We'll make this function two different ways: explicitly with a global and then a second time with a Python cache."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ba9ff6-6c14-45ac-acd5-bdd33165d627",
   "metadata": {},
   "source": [
    "### Explicit Globals\n",
    "The strategy here is to have our initialization function cache its state in a global.\n",
    "If that state exists, it will return it rather than re-initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48ece2e3-1ab1-4178-8f3d-bb35c33b6938",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffe0c8cd-e3bd-4906-988a-82e4f1087ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    \"\"\"Initialize the state for the function\"\"\"\n",
    "    global state\n",
    "    if state is None:\n",
    "        sleep(2)  # Faking an expensive function\n",
    "        state = 5\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57d5fa55-19c6-43f4-aa19-e396adac04ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.7 ms, sys: 894 µs, total: 13.6 ms\n",
      "Wall time: 2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5a56ac-9b92-4a4b-93ec-41566a2bc272",
   "metadata": {},
   "source": [
    "The first time running it will take a few seconds. But, then it's fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7785d90e-c281-4022-9545-70c04e37dc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13 µs, sys: 3 µs, total: 16 µs\n",
      "Wall time: 28.4 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96386c9c-3830-4b65-9671-37ccfdd2c841",
   "metadata": {},
   "source": [
    "We can use this inside our function to make it \"warmable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a570fc7c-6f24-4301-9e11-f22cc74fbe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(x):\n",
    "    weights = initialize()\n",
    "    return x + weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28a90bbe-4413-4690-83b3-a4340cbda625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 8.11 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "function(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35b8019-a516-4440-be2c-5a9d66c47ecc",
   "metadata": {},
   "source": [
    "It should be very fast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a40153-e90b-4b44-b62c-8aea097f4890",
   "metadata": {},
   "source": [
    "### Implicit Caching\n",
    "We can use an [LRU Cache](https://docs.python.org/3/library/functools.html#functools.lru_cache) to store the initialization value instead of making our own global.\n",
    "\n",
    "Python's LRU cache is created by decorating a function with the cache object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49301715-85ff-4ec2-ba05-8cbbb22761b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed75e5fb-cab8-4231-8b61-8df098f0e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache()\n",
    "def initialize():\n",
    "    sleep(2)\n",
    "    return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a2cf664-9a1f-44c9-b755-de92a1f9bc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.56 ms, sys: 299 µs, total: 1.86 ms\n",
      "Wall time: 2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "405b08d5-f83e-4911-8290-75208fa6a8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b536a690-5005-40b1-8b18-f1133f10a4a0",
   "metadata": {},
   "source": [
    "Like the other example, it is faster on the second time because it looks up the cache. This time, Python has created that cache for us. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b9f234-d716-4cf7-902c-64dc35455e71",
   "metadata": {},
   "source": [
    "### Initialization Changes with Inputs\n",
    "There are some cases where you may have a slightly-different initialization step based on the inputs (e.g., a path to a different neural network).\n",
    "LRU cache makes handling this case easy as it will return different results depending on the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70f036d9-68a7-4118-adc8-044f0e0d87a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=1)  # maxsize defines how many initializations to cache\n",
    "def initialize(model_id):\n",
    "    sleep(2)\n",
    "    return model_id    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a5340d-ec12-4cad-a703-06629a883ec0",
   "metadata": {},
   "source": [
    "Use it in a new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47f6ad66-1a8e-4800-9cce-263c96b11968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(model_id, x):\n",
    "    weights = initialize(model_id)\n",
    "    return x + weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a0934db-5837-43d4-b581-8f7d30c29dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.43 ms, sys: 225 µs, total: 1.65 ms\n",
      "Wall time: 2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "function(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a67badae-6b7c-4535-b8b5-892ddc175dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 6.44 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "function(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ef7e91b-baa5-48d6-8c05-4b0e7e79d6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 2.88 ms, total: 2.88 ms\n",
      "Wall time: 2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "function(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "147f529f-773e-4219-9bdd-1127d48e1404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
      "Wall time: 8.11 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "function(2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eaff4d-c543-4699-9367-f72a563cbcba",
   "metadata": {},
   "source": [
    "Note how re-using the same model does not incur the initialization cost, but changing models does!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47103db9-059a-458c-bd95-e15441237470",
   "metadata": {},
   "source": [
    "## Step 2: Build it in to module\n",
    "We are going to use our last strategy, LRU cache for the initiliazation, to make an example.\n",
    "\n",
    "We need to use a separate module for this function for it to work in a workflow engine. \n",
    "The workers used by this notebook need to know where to look for the definition of the function and the initialization function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10786738-1781-49ab-9143-a08fdf7828a2",
   "metadata": {},
   "source": [
    "I put them in a separate file [`module.py`](./module.py) in the same folder as this notebook, which is one of the places Python will look for the module definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4707e04-d9d8-4dcf-af59-c73def50c8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from module import function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d4fd274-b685-4439-bf91-bf25414b422d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 621 µs, sys: 135 µs, total: 756 µs\n",
      "Wall time: 2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "function(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a44520f4-271a-4aa4-8ad1-404ad354234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from module import function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71e46ed2-eb3d-4541-af0f-270413d9100a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "function(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0531a2-9273-4c4d-8090-d5533eb18859",
   "metadata": {},
   "source": [
    "It has our desired caching behavior because that function uses the same `initialize` function across multiple invocations even though we loaded the function twice (there is only one module)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1058252b-5d69-4ac2-8178-76804c3ad06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from module import initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00df7310-ff5c-4d8b-a2a3-c97d57f47eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CacheInfo(hits=1, misses=1, maxsize=1, currsize=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize.cache_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9097f58-7196-4488-b301-da4681720802",
   "metadata": {},
   "source": [
    "Note how the `initialize` function reports having run one and have re-used a cache value once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb3b9d4-1c4e-4998-8d7c-5a398564829f",
   "metadata": {},
   "source": [
    "And, because it is in a module, our workers will know how to access it. \n",
    "The [pickle library](https://docs.python.org/3/library/pickle.html) is how many workflow tools will send this function to workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68ed9b60-9706-44bf-b6e4-1135ad66ff14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x80\\x03cmodule\\nfunction\\nq\\x00.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "pkl.dumps(function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408b2c5d-e756-4387-be29-1b3cebbc0c71",
   "metadata": {},
   "source": [
    "Note how all it does is pass the location of the function (and not the cache)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834a2fb4-6ccb-47a1-a96a-947d4a9fdbf4",
   "metadata": {},
   "source": [
    "## Step 3: Show it off\n",
    "We'll use Python's [`ProcessPoolExecutor`](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor), which has a similar behavior to many workflow engines in that it passes data to persistent Python processes via pickle-serialized messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "418e5e9f-37a8-4a86-9a2b-a49b8ee71e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b71e36c0-629b-4bd0-a712-53760d3d57e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = ProcessPoolExecutor(1)  # Normally use this in the context-manager (with) syntax. I'm avoid that to show you some timings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fd196e-0f1e-4b17-9316-60a0001258dc",
   "metadata": {},
   "source": [
    "The first function invocation will take a few seconds as we're running initialization on the worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72490859-5fdb-4cfe-b074-939377547e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.54 ms, sys: 554 µs, total: 7.1 ms\n",
      "Wall time: 2.05 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ex.submit(function, 0, 1).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a080f87c-c5e3-444c-a164-86fcb79ba395",
   "metadata": {},
   "source": [
    "The second is fast because the worker has it's cache now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9bdabf7-6940-4239-9a25-e76d99ac3c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 947 µs, sys: 218 µs, total: 1.17 ms\n",
      "Wall time: 1.72 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ex.submit(function, 0, 1).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d30369e-641c-46f0-a4cd-88e5715ac603",
   "metadata": {},
   "source": [
    "You're done! We now have a function where each worker will avoid re-doing initialization work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c79de54a-3d25-4797-bb4d-95d550e94a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class X:\n",
    "    x = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c18a34-2ab6-4a4e-ba89-fdc5303b4451",
   "metadata": {},
   "source": [
    "## Advanced Notes\n",
    "There are a few thing we didn't address here:\n",
    "- *Configuration is Complex*: Instead of passing a lot of little arguments that define, bunch them all as a [`dataclass`](https://docs.python.org/3/library/dataclasses.html). Base the options on simple Python data types (e.g., `str`, `int`) so that `lru_cache` reliably detects when you've supplied the same configuration.\n",
    "- *Different Initialization on Different Hardware*: Some python functions require special commands to make them run quickly (e.g., placing PyTorch models on to the right device). My advice is to have your initialization function detect its environment based then act accordingly rather than adjust the source code between different workers or passing different arguments to the function to control how it works.\n",
    "- *De-initialization*: Some times you can only initialize one setting at a time (e.g., loading an array that fills up your whole systems' memory). Set your cache size to 1 in those cases (as in this example) and, if any additional work is required beyond letting Python free the memory as the cache is updated, consider using [`cachetools`](https://cachetools.readthedocs.io/) and modifying the `popitem` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e1f561-c072-4838-a527-0a0b0328ae28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
